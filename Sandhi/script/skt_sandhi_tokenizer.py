# -*- coding: utf-8 -*-
"""skt_sandhi_tokenizer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19A1BNyVxdcfqU5Kvgc_t729RLYZMw3Pu

https://www.tensorflow.org/text/guide/subwords_tokenizer
"""

n = 0
if n >= 0 and n < 10:
    work_dir = f'/sandhi/n{n}/'
# work_dir = '/content/drive/Othercomputers/MyComputer/ST/Sandhi/'

# !pip install -q -U tensorflow-text
# !pip install -q tensorflow_datasets

import collections
import os
import pathlib
import re
import string
import sys
import tempfile
import time

import numpy as np
import matplotlib.pyplot as plt

import tensorflow_datasets as tfds
import tensorflow_text as text
import tensorflow as tf

tf.get_logger().setLevel('ERROR')
pwd = pathlib.Path.cwd()

def split_text(n):
    if n == 0:
        fsname = work_dir + '/samhita.txt'
        fpname = work_dir + '/padapatha.txt'
    elif n > 0 and n < 6:
        fsname = work_dir + f'/samhita_{n}-gram.txt'
        fpname = work_dir + f'/padapatha_{n}-gram.txt'
    else:
        print('Error: n = 0, 1, 2, 3, 4, 5')
    
    with open(fsname, mode='r', encoding='utf-8') as fs, open(fpname, mode='r', encoding='utf-8') as fp:
        samhita_lines = [elem.rstrip() for elem in fs.readlines()]
        padapatha_lines = [elem.rstrip() for elem in fp.readlines()]
    
    dataset = np.array([samhita_lines[i] + ',' + padapatha_lines[i] for i in range(len(samhita_lines))])
    
    num_all = len(dataset)
    num_train = int(num_all * 0.8)
    num_test = num_all - num_train

    id_all = np.random.choice(num_all, num_all, replace=False)
    id_train = id_all[0:num_train]
    id_test = id_all[num_train:num_all]

    train_data = dataset[id_train]
    test_data = dataset[id_test]

    return {'train': train_data, 'eval': test_data}

tname = work_dir + 'train.csv'
vname = work_dir + 'val.csv'
with open(tname, mode='w', encoding='utf-8') as ft, open(vname, mode='w', encoding='utf-8') as fv:
    for lt in split_text(n)['train']:
        ft.write(lt + '\n')
    for lv in split_text(n)['eval']:
        fv.write(lv + '\n')

train_examples = tf.data.experimental.CsvDataset(tname, [tf.string, tf.string])
val_examples = tf.data.experimental.CsvDataset(vname, [tf.string, tf.string])

for samh, pada in train_examples.take(1):
  print("Samhitapatha: ", samh.numpy().decode('utf-8'))
  print("Padapatha:   ", pada.numpy().decode('utf-8'))

train_samh = train_examples.map(lambda samh, pada: pada)
train_pada = train_examples.map(lambda samh, pada: samh)

from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab

bert_tokenizer_params=dict(lower_case=False)
reserved_tokens=["[PAD]", "[UNK]", "[START]", "[END]"]

bert_vocab_args = dict(
    # The target vocabulary size
    vocab_size = 8000,
    # Reserved tokens that must be included in the vocabulary
    reserved_tokens=reserved_tokens,
    # Arguments for `text.BertTokenizer`
    bert_tokenizer_params=bert_tokenizer_params,
    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`
    learn_params={}
)

# Commented out IPython magic to ensure Python compatibility.
# %%time
samh_vocab = bert_vocab.bert_vocab_from_dataset(
    train_samh.batch(1000).prefetch(2),
    **bert_vocab_args
)

print(samh_vocab[:10])
print(samh_vocab[100:110])
print(samh_vocab[1000:1010])
print(samh_vocab[-10:])

def write_vocab_file(filepath, vocab):
  with open(filepath, 'w') as f:
    for token in vocab:
      print(token, file=f)

write_vocab_file(work_dir + 'samh_vocab.txt', samh_vocab)

# Commented out IPython magic to ensure Python compatibility.
# %%time
pada_vocab = bert_vocab.bert_vocab_from_dataset(
    train_pada.batch(1000).prefetch(2),
    **bert_vocab_args
)

print(pada_vocab[:10])
print(pada_vocab[100:110])
print(pada_vocab[1000:1010])
print(pada_vocab[-10:])

write_vocab_file(work_dir + 'pada_vocab.txt', pada_vocab)

# !ls *.txt

samh_tokenizer = text.BertTokenizer(work_dir + 'samh_vocab.txt', **bert_tokenizer_params)
pada_tokenizer = text.BertTokenizer(work_dir + 'pada_vocab.txt', **bert_tokenizer_params)

for smah_examples, pada_examples in train_examples.batch(3).take(1):
    for ex in pada_examples:
        print(ex.numpy())

# Tokenize the examples -> (batch, word, word-piece)
token_batch = pada_tokenizer.tokenize(pada_examples)
# Merge the word and word-piece axes -> (batch, tokens)
token_batch = token_batch.merge_dims(-2,-1)

# for ex in token_batch.to_list():
#   print(ex)

# Lookup each token id in the vocabulary.
txt_tokens = tf.gather(pada_vocab, token_batch)
# Join with spaces.
tf.strings.reduce_join(txt_tokens, separator=' ', axis=-1)

words = pada_tokenizer.detokenize(token_batch)
tf.strings.reduce_join(words, separator=' ', axis=-1)

START = tf.argmax(tf.constant(reserved_tokens) == "[START]")
END = tf.argmax(tf.constant(reserved_tokens) == "[END]")

def add_start_end(ragged):
  count = ragged.bounding_shape()[0]
  starts = tf.fill([count,1], START)
  ends = tf.fill([count,1], END)
  return tf.concat([starts, ragged, ends], axis=1)

words = pada_tokenizer.detokenize(add_start_end(token_batch))
tf.strings.reduce_join(words, separator=' ', axis=-1)

def cleanup_text(reserved_tokens, token_txt):
  # Drop the reserved tokens, except for "[UNK]".
  bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != "[UNK]"]
  bad_token_re = "|".join(bad_tokens)

  bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)
  result = tf.ragged.boolean_mask(token_txt, ~bad_cells)

  # Join them into strings.
  result = tf.strings.reduce_join(result, separator=' ', axis=-1)

  return result

pada_examples.numpy()

token_batch = pada_tokenizer.tokenize(pada_examples).merge_dims(-2,-1)
words = pada_tokenizer.detokenize(token_batch)
words

cleanup_text(reserved_tokens, words).numpy()

class CustomTokenizer(tf.Module):
  def __init__(self, reserved_tokens, vocab_path):
    self.tokenizer = text.BertTokenizer(vocab_path, lower_case=False)
    self._reserved_tokens = reserved_tokens
    self._vocab_path = tf.saved_model.Asset(vocab_path)

    vocab = pathlib.Path(vocab_path).read_text().splitlines()
    self.vocab = tf.Variable(vocab)

    ## Create the signatures for export:   

    # Include a tokenize signature for a batch of strings. 
    self.tokenize.get_concrete_function(
        tf.TensorSpec(shape=[None], dtype=tf.string))

    # Include `detokenize` and `lookup` signatures for:
    #   * `Tensors` with shapes [tokens] and [batch, tokens]
    #   * `RaggedTensors` with shape [batch, tokens]
    self.detokenize.get_concrete_function(
        tf.TensorSpec(shape=[None, None], dtype=tf.int64))
    self.detokenize.get_concrete_function(
          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))

    self.lookup.get_concrete_function(
        tf.TensorSpec(shape=[None, None], dtype=tf.int64))
    self.lookup.get_concrete_function(
          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))

    # These `get_*` methods take no arguments
    self.get_vocab_size.get_concrete_function()
    self.get_vocab_path.get_concrete_function()
    self.get_reserved_tokens.get_concrete_function()

  @tf.function
  def tokenize(self, strings):
    enc = self.tokenizer.tokenize(strings)
    # Merge the `word` and `word-piece` axes.
    enc = enc.merge_dims(-2,-1)
    enc = add_start_end(enc)
    return enc

  @tf.function
  def detokenize(self, tokenized):
    words = self.tokenizer.detokenize(tokenized)
    return cleanup_text(self._reserved_tokens, words)

  @tf.function
  def lookup(self, token_ids):
    return tf.gather(self.vocab, token_ids)

  @tf.function
  def get_vocab_size(self):
    return tf.shape(self.vocab)[0]

  @tf.function
  def get_vocab_path(self):
    return self._vocab_path

  @tf.function
  def get_reserved_tokens(self):
    return tf.constant(self._reserved_tokens)

tokenizers = tf.Module()
tokenizers.samh = CustomTokenizer(reserved_tokens, work_dir + 'samh_vocab.txt')
tokenizers.pada = CustomTokenizer(reserved_tokens, work_dir + 'pada_vocab.txt')

model_name = 'rigveda_samhitapatha_padapatha_converter'
tf.saved_model.save(tokenizers, work_dir + model_name)

